\section{Approach}
\label{sec:approach}

We will start by analyzing the benefits of a column store compared to a row store in an exhausting set of microbenchmarks, mainly based on Nexmark queries~\cite[]{tucker2008nexmark}.
The queries are selected in such a way that they only represent the relational algebra operation to be considered as exclusively as possible. 
In a separate step for each operation, we will then further streamline the selected query to the extent that it contains only the operation and is thus not influenced by any external factors.

After discussing the benefits, we will continue with the cost of converting to column format before the actual query, as well as converting back to a row format at the end of it.
Finally, we will also explore a few side strands for a few selected interesting and representative queries, which might influence the row and column store performance.

As a  result, we can hopefully predict at query compilation time whether an \ac{spe} will process data more efficiently row-based, column-based, or in a hybrid matter.
Finally, we test the suitability of non-row-store streaming in the Darwin stream processing engine~\cite[]{DBLP:conf/cidr/BensonR22}.

\subsection{Microbenchmark Analysis - Benefits}
Relational algebra boils down to the following four query primitives: \emph{Projections}, \emph{Selections}, \emph{Aggregations}, and \emph{Joins}.
We will approach them one by one, in the aforementioned order.
Across the operator experiments, we assume tumbling windows.
Orthogonally, we will consider for each primitive at least the influence of the data schema structure and the size of the input nano-batches.
Additionally, we will analyze operator-specific characteristics.

For each operator, we select a representative Nexmark~\cite{tucker2008nexmark} that isolates the effects of the relational algebra operator from other factors.
We will implement the processing of each operator once with data being ingested, processed, and egested as a row store and once with data being ingested, processed, and egested as a column store.
The queries will be hardcoded in C++.

\subsubsection{Projection}

A projection $\pi$ is the simplest operation from the primitives of relational algebra, which is why we will start with it.
It eliminates all attributes of the input relation but those mentioned in the projection list.

We select query 0 from the Apache Beam Nexmark benchmark suite extension~\cite[]{apachebeam}.
It looks as follows
\begin{lstlisting}[language=SQL]
SELECT auction, bidder, price, dateTime, extra FROM Bid;
\end{lstlisting}
The underlying schema looks as follows
\begin{lstlisting}[language=c++]
Bid(size_t auction, size_t bidder, size_t price, std::string channel, std::string url, time_t dateTime, std::string extra);
\end{lstlisting}
Already on this projection, we will analyze the influence of the nano-batch size, i.e. the tuples arriving at once.
Here we already hope for a better vectorization due to a larger number of available tuples.

In the next step, we will slightly modify the query so that all of the attributes of the input schema \emph{Bid} also occur in the output, instead of additionally filtering attributes.
As a result, the query represents a pure projection that is not influenced by any other operation and thus allows us to examine how unused attributes influence the query performance.
Working only on a few columns allows dropping entire arrays from an \ac{aos}, which should heavily favor the column store.
In addition, the query thus shows more balance between integers and strings.

Following, we start to modify the query to evaluate the influence of the schema on the projection.
To do this, we vary the number of fields of the input and output relations, as well as the size of the individual relations themselves.
The latter is controlled by modifying the length of the \emph{std::string} fields.

In addition, in a projection, calculations can also be performed on some attributes.
This time we use a modified form of query 1, which looks as follows
\begin{lstlisting}[language=SQL]
SELECT auction,  bidder, 0.908 * price, dateTime, extra FROM Bid;
\end{lstlisting}

To fully understand the influence of the actual computation, we also vary the query to remove all attributes from input and output that do not have to do with the actual calculation.
We further adapt this query during evaluation by replacing the floating point multiplication on the price with an integer multiplication, thus not getting a floating number as a result.
To further analyze the data types, we will replace the operation with an operation on an \emph{std::string}.
We also vary the complexity of the operation to understand its impact on performance.

\subsubsection{Selections}

A selection $\sigma$ is used for selecting a subset of the tuples according to a given selection condition.
We will use query 2 from the Apache Beam Nexmark benchmark suite extension to understand its impact on column and row stores. 
It represents a slightly modified query 2 from the original Nexmark queries since the original query will only yield a few hundred results over event streams of arbitrary size.
To make it more interesting, they instead choose a modulo operation for filtering.
It looks as follows
\begin{lstlisting}[language=SQL]
SELECT auction, price FROM Bid WHERE MOD(auction, 123) = 0;
\end{lstlisting}

Again, we are looking at the size of the nano-batches, tuples that can be processed at once.
Here we expect that an increased number of tuples leads to a much better vectorization on the hot filter attribute \emph{auction} and thus significantly improves the query performance.
Unique to the Selection investigation, however, is the filter condition, which we will look at in detail: Specifically, we will look at \textbf{(a)}, the selectivity of the filter, \textbf{(b)} the data type of the column that is being filtered on, and \textbf{(c)} what impact filters that span multiple columns have.

\subsubsection{Aggregations}
Aggregations are an umbrella term for the following five aggregate functions: \emph{Sum}, \emph{Count}, \emph{Average}, \emph{Maximum}, and \emph{Minimum}.
While they differ in the concrete calculation of the aggregates, they share that first, a grouping on a given attribute must take place.

Unfortunately, Nexmark does not provide a query that is exclusively an aggregation.
Therefore, we use the subquery of query 5 to represent an aggregation.
It consists only of a \emph{Count}, and thus the most CPU resource-friendly operation.
It looks as follows
\begin{lstlisting}[language=SQL]
SELECT auction, count(*) AS num FROM Bid GROUP BY auction;
\end{lstlisting}

To fully isolate the effect of the aggregation

We will further investigate the influence of the actual aggregation by adapting the aforementioned query for all four other aggregation functions accordingly. 

\subsubsection{Join}

A join $\theta$ is an operation that combines two relations with respect to a condition.
Thus, it is the only one of the operations considered that necessarily requires more than one column, which is why we consider it last.

This time we use Nexmark query 3, which looks as follows:
\begin{lstlisting}[language=SQL]
SELECT
    P.name, P.city, P.state, A.id
FROM
    auction AS A INNER JOIN person AS P on A.seller = P.id
WHERE
    A.category = 10 and (P.state = 'OR' OR P.state = 'ID' OR P.state = 'CA');
\end{lstlisting}
with
\begin{lstlisting}[language=c++]
Auction(size_t id, std::string itemName, std::string description, size_t initialId, size_t reserve, time_t dateTime, time_t expires, size_t seller, size_t category, std::string extra);
\end{lstlisting}
and

\begin{lstlisting}[language=c++]
Person(size_t id, std::string name, std::string emailAddress, std::string creditCard, std::string city, std::string state, time_t dateTime, std::string extra);
\end{lstlisting}

Besides again investigating the influence of the nano-batch size as well as the schema structure, a unique parameter to joins is the likelihood of finding a join partner.
Since the Nexmark queries do not come up with ranges for the attributes in the schema, we have to predict values and distributions.

Again, we modify the query 
\subsection{Microbenchmark Analysis - Costs}

However, using a column store does not come without a cost.
The tuples must first be converted from a row-based format to a column-based format upon arrival in the system, whereas with row-based processing a bare copy of the corresponding memory areas is sufficient.
As a final step after the query finished processing a tuple, it has egested as a tuple again, i.e. transformed back into a row representation.
Again, a bare copy of the corresponding memory areas is sufficient when doing row-based processing.

\subsubsection{Input Transformation}
The additional work of data transformation from a row-based to a column-based format also affects regular DBMS during data ingestion.
However, in a DBMS the data is read again multiple times after ingestion since multiple queries are expected to be executed on the same data.
This is usually not the case with \acp{spe}, where data is written once and only read once again.

At this point, we compare the transformation of all Nexmark schemas (\emph{Auction}, \emph{Bid}, and \emph{Person}) from a row-based to a column-based format.
The baseline against which we compare performance is a transformation of the schemas from a row-based to a row-based format, which can be represented by a \emph{memcpy}.


At this point, we expect again particular influences from the scheme as well as the size of the nano-batches arriving in the system.
While larger nano-batches should allow transforming the data from a row store into a column store more efficiently, many attributes could hinder the performance since during the split-up of the input tuple many output arrays have to be accessed.

\subsubsection{Output Transformation}

The same transformation that happens at the beginning of each query has to happen vice-versa at the end of it.
Since at this point we have looked at queries 0, 1, 2, 3, and 5, we will also look at conversion from their result schemas.
Those are

\begin{lstlisting}[language=c++]
Query0(size_t auction, size_t bidder, size_t price, time_t dateTime, time_t expires, std::string extra);
\end{lstlisting}

\begin{lstlisting}[language=c++]
Query1(size_t auction, size_t bidder, double price, time_t dateTime, time_t expires, std::string extra);
\end{lstlisting}

\begin{lstlisting}[language=c++]
Query2(size_t auction, size_t price);
\end{lstlisting}
    
\begin{lstlisting}[language=c++]
Query3(std::string name, std::string city, std::string state, size_t id);
\end{lstlisting}

\begin{lstlisting}[language=c++]
Query5(size_t auction, size_t num);
\end{lstlisting}

The baseline will again be transforming data from a row-based to a row-based format which is most efficiently done by a \emph{memcpy}.

\subsection{Additional Influences}

While the aforementioned operators will be the focus of our analysis, we presume that there are additional factors that influence the performance of a row store as well as a column store for \acp{spe}.
Those are the influence of additional input parsing and combined transformation into the correct data layout, the explicit vectorization of code, examining different memory architectures, the influence of window types, and, if time permits, how multithreading affects the comparison.

\subsubsection{Explicit Input Parsing}
However, while our previous additional cost analysis expected tuples to be arriving in C structs, the exchange format between stream processing systems is usually somewhat different.
JSON~\cite{DBLP:journals/pvldb/PalkarABZ18,DBLP:journals/vldb/LangdaleL19,DBLP:journals/pvldb/LiKCGK17} and CSV~\cite{DBLP:journals/pvldb/MuhlbauerRSRK013,DBLP:conf/sigmod/0002LECK19,DBLP:journals/pvldb/StehleJ20} files, in particular, are taken for exchange, which then have to be parsed when the tuples arrive.
Thus we want to examine how far the advantage of the row-based processing when using advanced exchange formats compared to a simple \emph{memcpy} is forfeited.
We expect, however, that the tuple exchange format should still favor the row-based store when ingesting data.

\subsubsection{Explicit SIMD}

Previous research has shown that inserting explicit SIMD operations compared to relying on compiler auto-vectorization can further improve query performance significantly~\cite{DBLP:journals/pvldb/KerstenLKNPB18}.
At this point, we want to investigate to what extent this effect also applies to data that is already stored in a \ac{soa}, and thus should already be easy for compilers to auto-vectorize.

\subsubsection{Memory Architectures}

With different vectorizing extensions on different platforms especially different kinds of memory on different machines~\cite[]{bollmeier2021processor}, especially \ac{hbm}, the performance of row-store-based and column-store-based streaming could be influenced significantly.
For reference, the \ac{hbm} ARM systems can reach up to 3x sequential memory bandwidth compared to state-of-the-art Intel, AMD, and Power systems which peak around 200 GB/s~\cite[]{bollmeier2021processor}.
Therefore, we want to understand how these different kinds of memory influence row-store and column-store streaming.

\subsection{Window Types}

While we only work with tumbling windows up to this point, we will extend the space by windowing types.
Here's what we expect: While with tumbling windows each tuple is written once and read exactly once again, the situation is different for sliding windows. 
The slices created by stream slicing are now read again for each window the slices belong to.
This results in a write-once-read-many scenario much closer to conventional DBMS which should favor the column store.

\subsubsection{Multithreading}

While multithreading, in general, is orthogonal to our design space in theory, designing multithreading-aware memory accesses often has tripping hazards and is therefore often non-trivial.
Especially cache line reuse and invalidation, tend to be tricky in a multithreading scenario.
It is important to note, that, at this point, we do not plan to design a fully parallel column-based \ac{spe}.
The primary focus is to verify the design and ensure that the column-based \ac{spe} is also multithreading capable.

\subsection{Combined Costs}

Finally, after all the individual analyses of the row-store and column-store-based design, we will evaluate the combined end-to-end of costs of a few selected queries.

\subsection{Darwin Integration}

As a very last step, we test the suitability of non-row-store streaming in the Darwin stream processing engine~\cite[]{DBLP:conf/cidr/BensonR22}.
A complete integration, which then allows both row-based and column-based processing of the data, appears to be very maintenance-intensive.
Accordingly, if column-based processing only brings small advantages, we will only test its use in a prototype.
If it turns out that column-based processing is indeed decisively superior in \acp{spe}, the entire Darwin memory model should be adapted accordingly.
Provided that it turns out that there are queries where one has advantages with column-based processing as well as others with row-based processing, we will have to implement both storage models in Darwin.